{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DeFlow","text":"<p>A Lightweight Declarative Data Framework that build on the \ud83c\udfc3 Workflow package.</p> <p>I want to use this project is the real-world use-case for my \ud83c\udfc3 Workflow package that able to handle production data pipeline with the DataOps strategy.</p> <p>Warning</p> <p>This framework does not allow you to custom your pipeline yet. If you want to create you workflow, you can use the \ud83c\udfc3 Workflow package instead that already installed.</p> <p>In my opinion, I think it should not create duplicate workflow codes if I can write with dynamic input parameters on the one template workflow that just change the input parameters per use-case instead. This way I can handle a lot of logical workflows in our orgs with only metadata configuration. It called Metadata Driven Data Workflow.</p>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>pip install -U deflow\n</code></pre>"},{"location":"#contribute","title":"\ud83d\udcac Contribute","text":"<p>I do not think this project will go around the world because it has specific propose, and you can create by your coding without this project dependency for long term solution. So, on this time, you can open the GitHub issue on this project \ud83d\ude4c for fix bug or request new feature if you want it.</p>"},{"location":"changelog/","title":"Changelogs","text":""},{"location":"changelog/#latest-changes","title":"Latest Changes","text":""},{"location":"changelog/#006","title":"0.0.6","text":""},{"location":"changelog/#features","title":"Features","text":"<ul> <li> v2: update node deps rule.</li> <li> move get_node function to utils function.</li> <li> add data framework version 3.</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug fixes","text":"<ul> <li> adjust workflow config base on version change.</li> <li> update logging for test on ci 3.</li> <li> update logging for test on ci 2.</li> <li> update logging for test on ci.</li> <li> passing pipeline name with node name.</li> <li> change env var for testing on ci.</li> </ul>"},{"location":"changelog/#build-workflow","title":"Build &amp; Workflow","text":"<ul> <li> remove python dev version on tests workflow.</li> </ul>"},{"location":"changelog/#dependencies","title":"Dependencies","text":"<ul> <li> update version of ddeutil-workflow to 0.0.85.</li> <li> update ddeutil-workflow version to 0.0.80.</li> </ul>"},{"location":"changelog/#documentations","title":"Documentations","text":"<ul> <li> update readme file.</li> <li> update readme file.</li> </ul>"},{"location":"changelog/#005","title":"0.0.5","text":""},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li> update model abstraction module.</li> <li> update model abstraction module.</li> <li> add get_data utils.</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li> override path on flow object.</li> <li> adjust dotenv file on ci.</li> </ul>"},{"location":"changelog/#code-changes","title":"Code Changes","text":"<ul> <li> update model testcase.</li> <li> update abstract model for any data framework.</li> <li> adjust log format for pytest.</li> <li> move testpath to new designed.</li> <li> \u2b06 deps: bump sigstore/gh-action-sigstore-python from 3.0.0 to 3.0.1 (#5)</li> </ul>"},{"location":"changelog/#build-workflow_1","title":"Build &amp; Workflow","text":"<ul> <li> bump ddeutil-workflow from 0.0.77 to 0.0.78 (#3)</li> <li> bump python-dotenv from 1.1.0 to 1.1.1 (#4)</li> </ul>"},{"location":"changelog/#dependencies_1","title":"Dependencies","text":"<ul> <li> update version ddeutil-workflow==0.0.79.</li> </ul>"},{"location":"changelog/#documentations_1","title":"Documentations","text":"<ul> <li> update readme file.</li> </ul>"},{"location":"changelog/#004","title":"0.0.4","text":""},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li> add get_data on utils func.</li> <li> draft variable for multiple deploy across env.</li> <li> add ignore filter.</li> <li> add asset method on node model.</li> </ul>"},{"location":"changelog/#code-changes_1","title":"Code Changes","text":"<ul> <li> add code format for data framework version 1.</li> <li> add code format for data framework version 2.</li> </ul>"},{"location":"changelog/#dependencies_2","title":"Dependencies","text":"<ul> <li> update ddeutil-workflow and json-schema.</li> </ul>"},{"location":"changelog/#documentations_2","title":"Documentations","text":"<ul> <li> update readme file.</li> </ul>"},{"location":"changelog/#003","title":"0.0.3","text":""},{"location":"changelog/#highlight-features","title":"Highlight Features","text":"<ul> <li> add data framework version 2 with dynamic caller.</li> </ul>"},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li> add pipeline workflow call node operator.</li> <li> add node workflow tempalte and pipeline construct.</li> <li> draft data framework version 2.</li> <li> add options for update extras value.</li> <li> add draft workflow for data frameworkf version 2.</li> <li> add option method for allow to update extra parameters.</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"Bug fixes","text":"<ul> <li> use caller instead routing on data framework version 2.</li> <li> testcase not valid when upgrade ddeutil-workflow deps.</li> <li> remove root_path from config module.</li> </ul>"},{"location":"changelog/#code-changes_2","title":"Code Changes","text":"<ul> <li> pull json schema from workflow package.</li> <li> fix testcase for extract caller func.</li> <li> clean core workflow template on data framework version 01.</li> <li> change workflow steps on data framework version 01.</li> <li> update testcase and add override datetime.</li> </ul>"},{"location":"changelog/#build-workflow_2","title":"Build &amp; Workflow","text":"<ul> <li> bump typer from 0.15.4 to 0.16.0 (#2)</li> </ul>"},{"location":"changelog/#dependencies_3","title":"Dependencies","text":"<ul> <li> update ddeutil-workflow to 0.0.70.</li> <li> downgrade typer version to 0.15.4.</li> <li> upgrade typer version to 0.16.0.</li> <li> update version of ddeutil-workflow to 0.0.66.</li> </ul>"},{"location":"changelog/#documentations_3","title":"Documentations","text":"<ul> <li> update readme file.</li> <li> update desc on v1 models module.</li> <li> update mkdocs version content.</li> <li> update badges on gh readme file.</li> </ul>"},{"location":"changelog/#002","title":"0.0.2","text":""},{"location":"changelog/#highlight-features_1","title":"Highlight Features","text":"<ul> <li> change template location with version base.</li> </ul>"},{"location":"changelog/#features_4","title":"Features","text":"<ul> <li> add routing workflow for custom workflow layer.</li> <li> merge priority group workflow to stream workflow.</li> <li> add routing workflow func for dynamic version.</li> <li> create routing tasks.</li> <li> update models module.</li> <li> update tasks for version 1.</li> <li> draft dynamic workflow templates.</li> <li> change tasks function base on api version.</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"Bug fixes","text":"<ul> <li> change routing task function parameter.</li> <li> change env name that not valid.</li> </ul>"},{"location":"changelog/#code-changes_3","title":"Code Changes","text":"<ul> <li> upgrade testcase.</li> <li> split testcase base on version.</li> <li> update testcase for override workflow config.</li> </ul>"},{"location":"changelog/#build-workflow_3","title":"Build &amp; Workflow","text":"<ul> <li> bump typer from 0.15.2 to 0.15.3 (#1)</li> </ul>"},{"location":"changelog/#dependencies_4","title":"Dependencies","text":"<ul> <li> update fixed trace bug of ddeutil-workflow version to 0.0.60.</li> <li> update ddeutil-workflow to 0.0.59</li> <li> update version of ddeutil-workflow from 0.0.51 to 0.0.58.</li> <li> update ddeutil-workflow to 0.0.56.</li> <li> update version of ddeutil-workflow from 0.0.42 to 0.0.51.</li> <li> update ddeutil-workflow from 0.0.41 to 0.0.42.</li> </ul>"},{"location":"changelog/#documentations_4","title":"Documentations","text":"<ul> <li> update mkdocs for making version topics.</li> <li> update mkdoc on data framework version 1.</li> <li> update design docs.</li> <li> update readme file.</li> <li> update readme file.</li> <li> update mkdocs package for making docs.</li> </ul>"},{"location":"changelog/#001","title":"0.0.1","text":""},{"location":"changelog/#highlight-features_2","title":"Highlight Features","text":"<ul> <li> add get_process function on io module.</li> </ul>"},{"location":"changelog/#features_5","title":"Features","text":"<ul> <li> add typer package for make cli app.</li> <li> update models for declarative framework.</li> <li> add prefix getting on config.</li> <li> add flow module and config.</li> <li> restructure code.</li> <li> add the first draft code of this project.</li> </ul>"},{"location":"changelog/#bug-fixes_4","title":"Bug fixes","text":"<ul> <li> add Self type-hint.</li> </ul>"},{"location":"changelog/#code-changes_4","title":"Code Changes","text":"<ul> <li> change prefix name support new package name.</li> <li> add dotenv setting func on conf testcase.</li> <li> Initial commit</li> </ul>"},{"location":"changelog/#deprecate-clean","title":"Deprecate &amp; Clean","text":"<ul> <li> rename package from fastflow to deflow.</li> </ul>"},{"location":"changelog/#build-workflow_4","title":"Build &amp; Workflow","text":"<ul> <li> add .github workflows.</li> <li> add hatchling to build system.</li> <li> add pyproject file.</li> </ul>"},{"location":"changelog/#dependencies_5","title":"Dependencies","text":"<ul> <li> update ddeutil-workflow from 0.0.40 to 0.0.41.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"Name Component Default Description DEFLOW_CORE_CONF_PATH CORE <code>./conf</code> A config path to get data framework configuration. DEFLOW_CORE_VERSION CORE <code>v1</code> A specific data framework version. DEFLOW_CORE_REGISTRY_CALLER CORE <code>.</code> A registry of caller function. <p>Support data framework version:</p> Version Supported Description 1 Progress A data framework that base on <code>stream</code>, <code>group</code>, and <code>process</code>. 2 Progress A data framework that base on <code>pipeline</code>, and <code>node</code>."},{"location":"design/","title":"Design","text":"<p>For governance part, I will give you a simple data pipeline naming design, and you can this be the baseline for the next generation.</p>"},{"location":"design/#stream","title":"Stream","text":"<pre><code>s_&lt;system&gt;_&lt;sub-domain?&gt;_&lt;frequency&gt;\n</code></pre> <pre><code>s_cm_d\n</code></pre> <p>Monitoring:</p> <ul> <li>Able to know want data that this stream handle, it handles <code>cm</code> data source.</li> <li>Able to know workflow frequency, this run with <code>daily</code> frequency.</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#initialize","title":"Initialize","text":"<p>Start initialize local data framework with the <code>init</code> CLI.</p> <pre><code>deflow init\n</code></pre> <p>The output project structure that was auto created from the initialize command:</p> <pre><code>conf/\n \u251c\u2500 conn/\n \u2502   \u251c\u2500 c_conn_01.yml\n \u2502   \u2570\u2500 c_conn_02.yml\n \u2570\u2500 stream/\n     \u2570\u2500 s_stream_01/\n         \u251c\u2500 g_group_01.tier.priority/\n         \u2502   \u251c\u2500 p_proces_01.yml\n         \u2502   \u2570\u2500 p_proces_02.yml\n         \u251c\u2500 g_group_02.tier.priority/\n         \u2502   \u251c\u2500 p_proces_01.yml\n         \u2502   \u2570\u2500 p_proces_02.yml\n         \u2570\u2500 s_stream_01.yml\n</code></pre>"},{"location":"getting-started/#running-flow","title":"Running Flow","text":"<pre><code>from deflow.flow import Flow\n\nflow = Flow(name=\"s_stream_name_d\").run(mode=\"N\")\n</code></pre>"},{"location":"api/flow/","title":"Flow","text":""},{"location":"metadata/domains/address/","title":"Address","text":""},{"location":"metadata/domains/customer/","title":"Customer Profile","text":""},{"location":"metadata/domains/customer/#profile","title":"Profile","text":"Field Name Alias Data Type PK Description <code>customer_id</code> cust_id STRING Y Unique identifier for the customer <code>first_name</code> fname STRING Customer's first name <code>last_name</code> lname STRING Customer's last name <code>email</code> email STRING Customer's email address <code>phone_number</code> phone STRING Customer's phone number <code>dob</code> dob DATE Date of birth <code>gender</code> gender STRING Customer's gender (e.g., \"M\", \"F\", \"Other\") <code>address</code> address STRING Full address (e.g., street, city, state, zip) <code>register_date</code> register DATE Date the customer signed up <code>email_opt_in</code> BOOLEAN Whether the customer opted in for email communications <code>sms_opt_in</code> BOOLEAN Whether the customer opted in for SMS communications"},{"location":"metadata/domains/customer/#family","title":"Family","text":"Field Name Alias Data Type PK Description <code>family_id</code> fam_id STRING Y Unique identifier for the family <code>family_name</code> fam_name STRING Family's name (e.g. last name of customer) <code>customer_id</code> cust_id STRING Identifier for the customer that stay on this family"},{"location":"metadata/structures/etl/","title":"ETL","text":"<p>The table that use the ETL process that ingest data to it should has process tracking field for tracking this record ingest from which process.</p> Field Name Alias Data Type PK Description src_name src_nm STRING load_date load_date DATETIME update_src_name update_src_nm STRING update_load_date update_load_date DATETIME"},{"location":"metadata/structures/etl/#delete-before","title":"Delete before","text":"<pre><code>DELECT FROM {target-schema}.{target-table-name}\nWHERE\n        src_name  =  '{src-name}'\n    AND load_date &gt;= '{current-date}'\n</code></pre>"},{"location":"metadata/structures/master/","title":"Master","text":"Field Name Alias Data Type PK Description business_id buz_id STRING Y Business key that is the unique key created_at created DATETIME Timestamp when the data was created updated_at updated DATETIME Timestamp when the data was last updated"},{"location":"metadata/structures/master/#scd1-full-dump","title":"SCD1 (Full-Dump)","text":"Field Name Alias Data Type PK Description business_id buz_id STRING Y Business key that is the unique key deleted_flag deleted_f BOOLEAN Delete flag that the data does not available created_at created DATETIME Timestamp when the data was created updated_at updated DATETIME Timestamp when the data was last updated"},{"location":"metadata/structures/master/#scd2","title":"SCD2","text":"Field Name Alias Data Type PK Description id id STRING Y Surrogate key that unique per row of data business_id buz_id STRING Business key that is the identify key for data when you fiter active_flag equal true active_flag active_f BOOLEAN Active flag that the data is the current update start_date valid_start DATETIME Timestamp when the data start to use end_date valid_end DATETIME Timestamp when the data end to use"},{"location":"metadata/structures/transaction/","title":"Transaction","text":"Field Name Alias Data Type PK Description business_id buz_id STRING Y Business key that is the unique key created_at created TIMESTAMP Y Timestamp when the transaction was created"},{"location":"version/v1/","title":"Version 1","text":"<p>A Data Framework Version 1. This concept use workflow to pass running date on all process that config in its stream.</p> <pre><code>core\n \u2570\u2500 stream\n     \u2570\u2500 group\n         \u2570\u2500 process\n</code></pre> <p>Note</p> <p>The audit date control by the steam layer and passing thai audit date to all its process after prepare with its frequency.</p>"},{"location":"version/v1/#concept","title":"Concept","text":"<pre><code>flowchart TD\n    start[Start] --&gt; getStreamInfo[\"Get stream information\\n(get-stream)\"]\n    getStreamInfo --&gt; startStream[\"Start stream\\n(start-stream)\"]\n    startStream --&gt; priorityLoop[\"For each priority in priority-groups\"]\n\n    priorityLoop --&gt; getGroups[\"Get Groups from Priority\\n(get-groups)\"]\n    getGroups --&gt; groupLoop[\"For each group in groups\"]\n\n    groupLoop --&gt; triggerGroupWorkflow[\"Trigger group-workflow\\nParams: name, stream, audit-date\"]\n    triggerGroupWorkflow --&gt; endTriggerGroup[\"End trigger Group\"]\n    endTriggerGroup --&gt; groupLoopCheck{\"More groups?\"}\n\n    groupLoopCheck -- Yes --&gt; groupLoop\n    groupLoopCheck -- No --&gt; endTriggerPriorityGroup[\"End trigger Priority Group\"]\n\n    endTriggerPriorityGroup --&gt; priorityLoopCheck{\"More priorities?\"}\n    priorityLoopCheck -- Yes --&gt; priorityLoop\n    priorityLoopCheck -- No --&gt; clearLog[\"Clear log\"]\n\n    clearLog --&gt; alert[\"Alert with mail or notify-services\"]\n    alert --&gt; finish[End]</code></pre>"},{"location":"version/v1/getting-started/","title":"Getting Started","text":"<p>Let design a retail data pipeline that exist in the common case worldwide.</p> <ul> <li>I have 3 data sources that store source data</li> <li>Customer master</li> <li>Product master</li> <li>Order transaction</li> <li>I want to ETL these sources to the bronze zone.</li> <li>Customer should load with Full-Dump extract</li> <li>Product master should load with Delta extract</li> <li>Order transaction should load with Transaction extract</li> <li>I will clean and create data model for these sources</li> <li>Customer dimension</li> <li>Product dimension</li> <li>Order fact</li> <li>Store dimension</li> <li>Finally, I will create serving table to be monthly reports</li> <li>Store selling report</li> <li>Product revenue report</li> </ul>"},{"location":"version/v1/metadata/","title":"Metadata","text":""},{"location":"version/v1/metadata/#schema-config","title":"Schema Config","text":""},{"location":"version/v1/metadata/#pipeline","title":"Pipeline","text":"Column Data Type PK Description pipeline_name STRING Y Pipeline name set_date INTEGER A number for decrease process date before running frequency STRING 'D', 'W', 'M', 'Q', 'Y' data_frequency STRING 'D', 'W', 'M', 'Q', 'Y' active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"version/v1/metadata/#node-group","title":"Node Group","text":"Column Data Type PK Description node_group_name STRING Y Node process group name pipeline_name STRING priority INTEGER active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"version/v1/metadata/#node","title":"Node","text":"Column Data Type PK Description node_name STRING Y Node process name node_group_name STRING priority INTEGER load_type STRING data_load_type STRING T, D, F, SCD1, SCD2_D, SCD2_F, SCD2_T extras JSON active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"version/v1/metadata/#node-dependency","title":"Node Dependency","text":"Column Data Type PK Description node_name STRING Y Node process name node_dependency_name STRING Y Node process dependency name dependency_set_date INTEGER A number for decrease dependency process date before running active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data <p>Note</p> <p>The <code>extras</code> field on the node table contain all of necessary node arguments;</p> <pre><code>extras\n  \u251c\u2500 files\n  \u2502   \u251c\u2500 name\n  \u2502   \u251c\u2500 path\n  \u2502   \u251c\u2500 header\n  \u2502   \u2570\u2500 encoding\n  \u251c\u2500 storage\n  \u2502   \u251c\u2500 system\n  \u2502   \u2570\u2500 container\n  \u2570\u2500 framwork\n      \u251c\u2500 archiving\n      \u2570\u2500 timeout\n</code></pre>"},{"location":"version/v1/metadata/#schema-data","title":"Schema Data","text":""},{"location":"version/v1/metadata/#pipeline-watermark-merge","title":"Pipeline Watermark (Merge)","text":"Column Data Type PK Description pipeline_name STRING Y Pipeline name process_date DATETIME Process datetime that be the latest process running next_process_date DATETIME previous_process_date DATETIME data_date DATETIME next_data_date DATETIME previous_data_date DATETIME running_flag BOOLEAN A running flag for tacking this pipeline is running or not update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"version/v1/metadata/#pipeline-logging-append-only","title":"Pipeline Logging (Append Only)","text":"Column Data Type PK Description pipeline_name STRING Y Pipeline name process_id INTEGER Y Process ID orchestrate_id INTEGER Y Orchestration tools running id process_ts TIMESTAMP process_date DATETIME status STRING 'Success', 'Failed', 'Start' log STRING tracking JSON update_stage STRING update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"version/v1/metadata/#node-logging-append-only","title":"Node Logging (Append Only)","text":"Column Data Type PK Description node_name STRING Y Node process name process_id INTEGER Y Process ID orchestrate_id INTEGER Y Orchestration tools running id process_ts TIMESTAMP Process timestamp process_date DATETIME Process datetime status STRING 'Success', 'Failed', 'Start' log STRING records JSON update_stage STRING update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data <p>Note</p> <p>The <code>records</code> field on the node log table contain all of necessary result records;</p> <pre><code>records\n  \u251c\u2500 count\n  \u2570\u2500 control_count\n</code></pre> <p>If you get the log <code>records</code> that not null and grouping its by <code>update_stage</code>, it will be tracing record like;</p> <pre><code>records\n  \u251c\u2500 src\n  \u2502   \u251c\u2500 count\n  \u2502   \u2570\u2500 control_count\n  \u251c\u2500 stg ...\n  \u251c\u2500 stg ...\n  \u2570\u2500 tgt\n      \u251c\u2500 count\n      \u2570\u2500 control_count\n</code></pre>"},{"location":"version/v1/metadata/#query","title":"Query","text":""},{"location":"version/v1/metadata/#node-information","title":"Node information","text":"<pre><code>SELECT\n      pipe.pipeline_name                    AS pipeline_name\n    , node_group.node_group_name            AS node_group_name\n    , node.*\nFROM control.control_pipeline               AS pipe\nJOIN control.control_node_group             AS node_group\n    ON  pipe.pipeline_name   = node_group.node_group_name\nJOIN control.control_node                   AS node\n    ON  node_group.node_name = node.node_name\nWHERE\n        pipe.active_flag        is true\n    AND node_group.active_flag  is true\n    AND node.active_flag        is true\n</code></pre>"},{"location":"version/v1/metadata/#node-log","title":"Node log","text":"<pre><code>SELECT\n    node_name\n    , process_id\n    , orchestrate_id\n    , MIN(process_date)                     AS start_date\n    , MAX(process_date)                     AS end_date\nFROM control.log_node\nGROUP BY\n    node_name\n    , process_id\n    , orchestrate_id\n</code></pre>"},{"location":"version/v1/metadata/#node-dependency-needed","title":"Node dependency needed","text":"<pre><code>SELECT\n    node_deps.node_dependency_name          AS node_deps_name\n    , CASE WHEN MAX(COALESCE(log.process_id, -999)) = -999 THEN 'Need'\n           ELSE 'Pass'\n      END                                   AS latest_process_id\nFROM control.control_node                   AS node\nINNER JOIN control.control_node_dependency  AS node_deps\n    ON node.node_name   = node_deps.node_name\nLEFT JOIN control.log_node                  AS log\n    ON log.node_name    = node_deps.node_dependency_name\nWHERE\n        log.status       = 'Success'\n    AND '{process-date}' = DATEADD(DAY, node_deps.dependency_set_date, log.process_date)\nGROUP BY node_deps.node_dependency_name\n</code></pre>"},{"location":"version/v2/","title":"Version 2","text":"<p>A Data Framework Version 2. This concept use workflow to pass running date on only node only.</p> <pre><code>core\n \u2570\u2500 pipeline\n     \u2570\u2500 node\n</code></pre> <p>Note</p> <p>The audit date control by the node layer that mean the audit date can different between node in its same pipeline.</p>"},{"location":"version/v2/#concept","title":"Concept","text":""},{"location":"version/v2/metadata/","title":"Metadata","text":""},{"location":"version/v2/metadata/#schema-config","title":"Schema Config","text":""},{"location":"version/v2/metadata/#node","title":"Node","text":"Column Data Type PK Description node_name STRING Y Node process name node_group_name STRING priority INTEGER load_type STRING data_load_type STRING T, D, F, SCD1, SCD2_D, SCD2_F, SCD2_T extras JSON active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data"},{"location":"version/v2/metadata/#node-dependency","title":"Node Dependency","text":"Column Data Type PK Description node_name STRING Y Node process name node_dependency_name STRING Y Node process dependency name dependency_set_date INTEGER A number for decrease dependency process date before running active_flag BOOLEAN Active flag for this config data update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data <p>Note</p> <p>The <code>extras</code> field on the node table contain all of necessary node arguments;</p> <pre><code>extras\n  \u251c\u2500 files\n  \u2502   \u251c\u2500 name\n  \u2502   \u251c\u2500 path\n  \u2502   \u251c\u2500 header\n  \u2502   \u2570\u2500 encoding\n  \u251c\u2500 storage\n  \u2502   \u251c\u2500 system\n  \u2502   \u2570\u2500 container\n  \u2570\u2500 framwork\n      \u251c\u2500 archiving\n      \u2570\u2500 timeout\n</code></pre>"},{"location":"version/v2/metadata/#schema-data","title":"Schema Data","text":""},{"location":"version/v2/metadata/#node-logging","title":"Node Logging","text":"Column Data Type PK Description node_name STRING Y Node process name process_id INTEGER Y start_date DATETIME end_date DATETIME process_date DATETIME status STRING Success, Failed, Start log STRING records JSON update_by STRING Who that add or update this config data update_date DATETIME Update datetime of this config data <p>Note</p> <p>The <code>records</code> field on the node log table contain all of necessary result records;</p> <pre><code>records\n  \u251c\u2500 src\n  \u2502   \u251c\u2500 count\n  \u2502   \u2570\u2500 control_count\n  \u2570\u2500 tgt\n      \u251c\u2500 count\n      \u2570\u2500 control_count\n</code></pre>"},{"location":"version/v3/","title":"Version 3","text":""}]}